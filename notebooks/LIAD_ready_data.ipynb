{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332ac2df",
   "metadata": {},
   "source": [
    "### Processing LIAD-Framework ready data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e519da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from joblib import Parallel, delayed\n",
    "import math\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "\n",
    "def split_by_time_bins(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    df['num_days'] = (df['finished_at'].dt.normalize() - df['started_at'].dt.normalize()).dt.days + 1\n",
    "    \n",
    "    df_exploded = df.loc[df.index.repeat(df['num_days'])].copy()\n",
    "    df_exploded['day_offset'] = df_exploded.groupby(level=0).cumcount()\n",
    "    \n",
    "    df_exploded['current_day_midnight'] = df_exploded['started_at'].dt.normalize() + pd.to_timedelta(df_exploded['day_offset'], unit='D')\n",
    "    \n",
    "    df_exploded['started_at'] = df_exploded[['started_at', 'current_day_midnight']].max(axis=1)\n",
    "    df_exploded['day_end_boundary'] = df_exploded['current_day_midnight'] + pd.to_timedelta(1, unit='D')\n",
    "    df_exploded['finished_at'] = df_exploded[['finished_at', 'day_end_boundary']].min(axis=1)\n",
    "    \n",
    "    df_daily = df_exploded[df_exploded['started_at'] < df_exploded['finished_at']].reset_index(drop=True)\n",
    "\n",
    "    time_bins = [\n",
    "        ('00:00:00', '06:00:00', 'Early Morning'),  # 0:00 - 5:59\n",
    "        ('06:00:00', '09:00:00', 'Morning Rush'),   # 6:00 - 8:59\n",
    "        ('09:00:00', '14:00:00', 'Mid Day'),        # 9:00 - 13:59\n",
    "        ('14:00:00', '17:30:00', 'Afternoon'),      # 14:00 - 17:29\n",
    "        ('17:30:00', '21:30:00', 'Evening'),        # 17:30 - 21:29\n",
    "        ('21:30:00', '1 day',    'Night')           # 21:30 - 23:59\n",
    "    ]\n",
    "    \n",
    "    final_segments = []\n",
    "\n",
    "    for start_str, end_str, label in time_bins:\n",
    "        temp_df = df_daily.copy()\n",
    "        \n",
    "        bin_start_delta = pd.to_timedelta(start_str)\n",
    "        bin_end_delta = pd.to_timedelta(end_str)\n",
    "        \n",
    "        bin_abs_start = temp_df['current_day_midnight'] + bin_start_delta\n",
    "        bin_abs_end = temp_df['current_day_midnight'] + bin_end_delta\n",
    "        \n",
    "        temp_df['started_at'] = pd.concat([temp_df['started_at'], bin_abs_start], axis=1).max(axis=1)\n",
    "        temp_df['finished_at'] = pd.concat([temp_df['finished_at'], bin_abs_end], axis=1).min(axis=1)\n",
    "        \n",
    "        valid_segments = temp_df[temp_df['started_at'] < temp_df['finished_at']]\n",
    "        final_segments.append(valid_segments)\n",
    "\n",
    "    df_split = pd.concat(final_segments).sort_values(by=['started_at']).reset_index(drop=True)\n",
    "    \n",
    "    cols_to_drop = ['num_days', 'day_offset', 'current_day_midnight', 'day_end_boundary']\n",
    "    df_split = df_split.drop(columns=[c for c in cols_to_drop if c in df_split.columns])\n",
    "\n",
    "    return df_split\n",
    "\n",
    "def assign_time_segment(dt):\n",
    "    \"\"\"\n",
    "    Assign time segment based on hour and minute.\n",
    "    Segments: 0-5.59, 6-8.59, 9-13.59, 14-17.29, 17.30-21.29, 21.30-23.59\n",
    "    \"\"\"\n",
    "    hour = dt.hour\n",
    "    minute = dt.minute\n",
    "    \n",
    "    if hour < 6:\n",
    "        return '0-5.59'\n",
    "    elif hour < 9:\n",
    "        return '6-8.59'\n",
    "    elif hour < 14:\n",
    "        return '9-13.59'\n",
    "    elif hour < 17 or (hour == 17 and minute < 30):\n",
    "        return '14-17.29'\n",
    "    elif hour < 21 or (hour == 21 and minute < 30):\n",
    "        return '17.30-21.29'\n",
    "    else:\n",
    "        return '21.30-23.59'\n",
    "    \n",
    "\n",
    "def merge_consecutive_locations(\n",
    "    df: pd.DataFrame,\n",
    "    agent_col: str = \"agent\",\n",
    "    loc_col: str = \"location_id\",\n",
    "    start_col: str = \"started_at\",\n",
    "    end_col: str = \"finished_at\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each agent, merge consecutive rows with the same location_id into a single row.\n",
    "    Keeps:\n",
    "      - started_at = first started_at of the run\n",
    "      - finished_at = last finished_at of the run\n",
    "      - latitude/longitude/poi_category = first value in the run (change if you want)\n",
    "    Assumes rows are ordered by time per agent; we enforce it via sort.\n",
    "    \"\"\"\n",
    "\n",
    "    out = df.copy()\n",
    "    out[start_col] = pd.to_datetime(out[start_col], errors=\"coerce\")\n",
    "    out[end_col]   = pd.to_datetime(out[end_col], errors=\"coerce\")\n",
    "\n",
    "    out = out.sort_values([agent_col, start_col, end_col], kind=\"mergesort\")\n",
    "\n",
    "    new_run = out[loc_col].ne(out.groupby(agent_col)[loc_col].shift())\n",
    "\n",
    "    out[\"_run_id\"] = new_run.groupby(out[agent_col]).cumsum()\n",
    "    merged = (\n",
    "        out.groupby([agent_col, \"_run_id\"], sort=False, as_index=False)\n",
    "           .agg(\n",
    "               started_at=(start_col, \"first\"),\n",
    "               finished_at=(end_col, \"last\"),\n",
    "               latitude=(\"latitude\", \"first\"),\n",
    "               longitude=(\"longitude\", \"first\"),\n",
    "               location_id=(loc_col, \"first\"),\n",
    "               poi_category=(\"poi_category\", \"first\"),\n",
    "           )\n",
    "           .drop(columns=\"_run_id\")\n",
    "    )\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e61c1d",
   "metadata": {},
   "source": [
    "### Train / Test data Processing\n",
    "\n",
    "Train and Test data should contain:\n",
    "\n",
    "1. \"agent\"        : Id of the agent\n",
    "\n",
    "2. \"started_at\".  : Date and time. We assume the data you provide are UTC, then we convert to Asia/Tokyo.\n",
    "\n",
    "3. \"finished_at\"  : Date and time. We assume the data you provide are UTC, then we convert to Asia/Tokyo.\n",
    "\n",
    "4. \"location_id\"  : A unique Id of the location (we use this to uniquely define the agent's home)\n",
    "\n",
    "5. \"latitude\"       \n",
    "\n",
    "6. \"longitude\"\n",
    "\n",
    "7. \"poi_category\" : POI label of the location\n",
    "\n",
    "Store the data as train.csv and test.csv in the processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f3137e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some of our data processing steps may or may not be important for you\n",
    "\n",
    "train = '/Users/chanuka/Desktop/codespaces/liad/data/sim2_evalb/sim2_evalb_stay_points_train.parquet'\n",
    "test = '/Users/chanuka/Desktop/codespaces/liad/data/sim2_evalb/sim2_evalb_stay_points_test.parquet'\n",
    "\n",
    "train_data = pd.read_parquet(test)\n",
    "train_data['duration'] = (pd.to_datetime(train_data['end_datetime']) - pd.to_datetime(train_data['start_datetime'])).dt.total_seconds() / 60\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5107231",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data['duration'] > 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ad61f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rename(columns={'agent_id': 'agent', 'start_datetime': 'started_at', 'end_datetime': 'finished_at', 'latitude_sp': 'latitude', 'longitude_sp': 'longitude', 'poi_id': 'location_id', 'category': 'poi_category'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0886dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(columns=['distance_m', 'duration'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aed10fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = merge_consecutive_locations(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9deac6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/sim2_evalb/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(test)\n",
    "\n",
    "eval_b_train = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_stay_points_test_anomalous.parquet')\n",
    "poi_data = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_poi.parquet')\n",
    "eval_b_train = eval_b_train.merge(poi_data, on='poi_id', how='left')\n",
    "eval_b_train\n",
    "\n",
    "eval_b_train.rename(columns={'agent_id': 'agent', 'start_datetime':'started_at', 'end_datetime':'finished_at', 'poi_id': 'location_id', 'category':'poi_category'}, inplace=True)\n",
    "eval_b_train.drop(columns=['source', 'anomaly_type', 'anomaly'], inplace=True)\n",
    "\n",
    "gt = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail_4_sim1_fis/anomalous_temporal_sim1.csv')\n",
    "ground_truth = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail_4_sim1_fis/anomalous_agent_sim1.csv')\n",
    "\n",
    "gt\n",
    "\n",
    "eval_b_train = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_stay_points_test_anomalous.parquet')\n",
    "eval_b_train = eval_b_train[eval_b_train['anomaly'] == True]\n",
    "eval_b_train.rename(columns={'agent_id': 'agent', 'start_datetime': 'anomaly_start_time', 'end_datetime': 'anomaly_end_time'}, inplace=True)\n",
    "eval_b_train.to_csv('../processed/anomalous_temporal.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9be9c",
   "metadata": {},
   "source": [
    "### Ground Truth Processing\n",
    "\n",
    "For the evaluation purpose, you need to have a anomalous_agents.csv saved in processed folder,\n",
    "the file ideally should contain a single column called 'agent', which contain the ids of all the anomalous agents in the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b1d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/processed/sim2_evalb/gt/TeamA_anomalous_temporal.csv')\n",
    "data2 = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/processed/sim2_evalb/gt/TeamB_anomalous_temporal.csv')\n",
    "data3 = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/processed/sim2_evalb/gt/TeamC_anomalous_temporal.csv')\n",
    "data4 = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/processed/sim2_evalb/gt/TeamD_anomalous_temporal.csv')\n",
    "\n",
    "gt = pd.concat([data1, data2, data3, data4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ad9b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.rename(columns={'event_start_time': 'started_at', 'event_end_time': 'finished_at'}, inplace=True)\n",
    "gt = gt[['agent', 'started_at', 'finished_at']]\n",
    "gt['started_at'] = pd.to_datetime(gt['started_at'])\n",
    "gt['finished_at'] = pd.to_datetime(gt['finished_at'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1684bef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.agent.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('/Users/chanuka/Desktop/codespaces/neural_reeb/data/trail_4_sim1_fis/anomalous_temporal_sim1.csv')\n",
    "gt_new = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_stay_points_test_anomalous.parquet')\n",
    "gt_new['start_datetime'] = pd.to_datetime(gt_new['start_datetime'], utc=True).dt.tz_convert('Asia/Tokyo')\n",
    "gt_new['end_datetime'] = pd.to_datetime(gt_new['end_datetime'], utc=True).dt.tz_convert('Asia/Tokyo')\n",
    "gt_new.rename(columns={'agent_id': 'agent', 'start_datetime': 'started_at', 'end_datetime': 'finished_at'}, inplace=True) # should have agent, started_at, finished_at\n",
    "gt_new = gt_new[gt_new['anomaly']== True]\n",
    "gt_new = gt_new[['agent', 'started_at', 'finished_at']]\n",
    "\n",
    "gt_new_agents = pd.DataFrame(gt_new.agent.unique(), columns=['agent'])\n",
    "\n",
    "# format 1\n",
    "gt_new_agents.to_csv('../processed/anomalous_agents.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e621b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = split_by_time_bins(gt)\n",
    "\n",
    "gt['time_segment'] = gt['started_at'].apply(assign_time_segment)\n",
    "gt['day_of_week'] = gt['started_at'].dt.dayofweek\n",
    "gt['day_type'] = gt['day_of_week'].apply(lambda x: 'weekend' if x >= 5 else 'weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75d284b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.to_csv('../processed/sim2_evalb/anomalous_segmented.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
