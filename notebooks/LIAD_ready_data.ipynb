{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332ac2df",
   "metadata": {},
   "source": [
    "### Processing LIAD-Framework ready data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e519da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from joblib import Parallel, delayed\n",
    "import math\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "\n",
    "def split_by_time_bins(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    df['num_days'] = (df['finished_at'].dt.normalize() - df['started_at'].dt.normalize()).dt.days + 1\n",
    "    \n",
    "    df_exploded = df.loc[df.index.repeat(df['num_days'])].copy()\n",
    "    df_exploded['day_offset'] = df_exploded.groupby(level=0).cumcount()\n",
    "    \n",
    "    df_exploded['current_day_midnight'] = df_exploded['started_at'].dt.normalize() + pd.to_timedelta(df_exploded['day_offset'], unit='D')\n",
    "    \n",
    "    df_exploded['started_at'] = df_exploded[['started_at', 'current_day_midnight']].max(axis=1)\n",
    "    df_exploded['day_end_boundary'] = df_exploded['current_day_midnight'] + pd.to_timedelta(1, unit='D')\n",
    "    df_exploded['finished_at'] = df_exploded[['finished_at', 'day_end_boundary']].min(axis=1)\n",
    "    \n",
    "    df_daily = df_exploded[df_exploded['started_at'] < df_exploded['finished_at']].reset_index(drop=True)\n",
    "\n",
    "    time_bins = [\n",
    "        ('00:00:00', '06:00:00', 'Early Morning'),  # 0:00 - 5:59\n",
    "        ('06:00:00', '09:00:00', 'Morning Rush'),   # 6:00 - 8:59\n",
    "        ('09:00:00', '14:00:00', 'Mid Day'),        # 9:00 - 13:59\n",
    "        ('14:00:00', '17:30:00', 'Afternoon'),      # 14:00 - 17:29\n",
    "        ('17:30:00', '21:30:00', 'Evening'),        # 17:30 - 21:29\n",
    "        ('21:30:00', '1 day',    'Night')           # 21:30 - 23:59\n",
    "    ]\n",
    "    \n",
    "    final_segments = []\n",
    "\n",
    "    for start_str, end_str, label in time_bins:\n",
    "        temp_df = df_daily.copy()\n",
    "        \n",
    "        bin_start_delta = pd.to_timedelta(start_str)\n",
    "        bin_end_delta = pd.to_timedelta(end_str)\n",
    "        \n",
    "        bin_abs_start = temp_df['current_day_midnight'] + bin_start_delta\n",
    "        bin_abs_end = temp_df['current_day_midnight'] + bin_end_delta\n",
    "        \n",
    "        temp_df['started_at'] = pd.concat([temp_df['started_at'], bin_abs_start], axis=1).max(axis=1)\n",
    "        temp_df['finished_at'] = pd.concat([temp_df['finished_at'], bin_abs_end], axis=1).min(axis=1)\n",
    "        \n",
    "        valid_segments = temp_df[temp_df['started_at'] < temp_df['finished_at']]\n",
    "        final_segments.append(valid_segments)\n",
    "\n",
    "    df_split = pd.concat(final_segments).sort_values(by=['started_at']).reset_index(drop=True)\n",
    "    \n",
    "    cols_to_drop = ['num_days', 'day_offset', 'current_day_midnight', 'day_end_boundary']\n",
    "    df_split = df_split.drop(columns=[c for c in cols_to_drop if c in df_split.columns])\n",
    "\n",
    "    return df_split\n",
    "\n",
    "def assign_time_segment(dt):\n",
    "    \"\"\"\n",
    "    Assign time segment based on hour and minute.\n",
    "    Segments: 0-5.59, 6-8.59, 9-13.59, 14-17.29, 17.30-21.29, 21.30-23.59\n",
    "    \"\"\"\n",
    "    hour = dt.hour\n",
    "    minute = dt.minute\n",
    "    \n",
    "    if hour < 6:\n",
    "        return '0-5.59'\n",
    "    elif hour < 9:\n",
    "        return '6-8.59'\n",
    "    elif hour < 14:\n",
    "        return '9-13.59'\n",
    "    elif hour < 17 or (hour == 17 and minute < 30):\n",
    "        return '14-17.29'\n",
    "    elif hour < 21 or (hour == 21 and minute < 30):\n",
    "        return '17.30-21.29'\n",
    "    else:\n",
    "        return '21.30-23.59'\n",
    "    \n",
    "\n",
    "def merge_consecutive_locations(\n",
    "    df: pd.DataFrame,\n",
    "    agent_col: str = \"agent\",\n",
    "    loc_col: str = \"location_id\",\n",
    "    start_col: str = \"started_at\",\n",
    "    end_col: str = \"finished_at\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each agent, merge consecutive rows with the same location_id into a single row.\n",
    "    Keeps:\n",
    "      - started_at = first started_at of the run\n",
    "      - finished_at = last finished_at of the run\n",
    "      - latitude/longitude/poi_category = first value in the run (change if you want)\n",
    "    Assumes rows are ordered by time per agent; we enforce it via sort.\n",
    "    \"\"\"\n",
    "\n",
    "    out = df.copy()\n",
    "    out[start_col] = pd.to_datetime(out[start_col], errors=\"coerce\")\n",
    "    out[end_col]   = pd.to_datetime(out[end_col], errors=\"coerce\")\n",
    "\n",
    "    out = out.sort_values([agent_col, start_col, end_col], kind=\"mergesort\")\n",
    "\n",
    "    new_run = out[loc_col].ne(out.groupby(agent_col)[loc_col].shift())\n",
    "\n",
    "    out[\"_run_id\"] = new_run.groupby(out[agent_col]).cumsum()\n",
    "    merged = (\n",
    "        out.groupby([agent_col, \"_run_id\"], sort=False, as_index=False)\n",
    "           .agg(\n",
    "               started_at=(start_col, \"first\"),\n",
    "               finished_at=(end_col, \"last\"),\n",
    "               latitude=(\"latitude\", \"first\"),\n",
    "               longitude=(\"longitude\", \"first\"),\n",
    "               location_id=(loc_col, \"first\"),\n",
    "               poi_category=(\"poi_category\", \"first\"),\n",
    "           )\n",
    "           .drop(columns=\"_run_id\")\n",
    "    )\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e61c1d",
   "metadata": {},
   "source": [
    "### Train / Test data Processing\n",
    "\n",
    "Train and Test data should contain:\n",
    "\n",
    "1. \"agent\"        : Id of the agent\n",
    "\n",
    "2. \"started_at\".  : Date and time. We assume the data you provide are UTC, then we convert to Asia/Tokyo.\n",
    "\n",
    "3. \"finished_at\"  : Date and time. We assume the data you provide are UTC, then we convert to Asia/Tokyo.\n",
    "\n",
    "4. \"location_id\"  : A unique Id of the location (we use this to uniquely define the agent's home)\n",
    "\n",
    "5. \"latitude\"       \n",
    "\n",
    "6. \"longitude\"\n",
    "\n",
    "7. \"poi_category\" : POI label of the location\n",
    "\n",
    "Store the data as train.csv and test.csv in the processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f3137e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_type_filter(train_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute total duration per agent on the train month and keep agents\n",
    "    above the 16th percentile (this needs to be customized depending on the data).\n",
    "    Works on an in-memory DataFrame (no extra csv read).\n",
    "    \"\"\"\n",
    "    tmp = train_df[['agent', 'started_at', 'finished_at']].copy()\n",
    "\n",
    "    tmp['started_at'] = pd.to_datetime(tmp['started_at'])\n",
    "    tmp['finished_at'] = pd.to_datetime(tmp['finished_at'])\n",
    "\n",
    "    tmp['duration_min'] = (tmp['finished_at'] - tmp['started_at']).dt.total_seconds() / 60.0\n",
    "    tmp['duration'] = tmp['duration_min'].clip(lower=0).fillna(0)\n",
    "\n",
    "    train_agent_dur = tmp.groupby('agent')['duration'].sum()\n",
    "\n",
    "    df = pd.DataFrame({'train_duration': train_agent_dur}).fillna(0)\n",
    "\n",
    "    q1_value = df['train_duration'].quantile(0.16)\n",
    "    df_top_q1 = df[df['train_duration'] >= q1_value]\n",
    "\n",
    "    return df_top_q1.index\n",
    "\n",
    "train = '../data/trail5/stop_past/agent_bucket=0.parquet'\n",
    "test = '../data/trail5/stop_future/agent_bucket=0.parquet'\n",
    "\n",
    "train_data = pd.read_parquet(train)\n",
    "test_data = pd.read_parquet(test)\n",
    "\n",
    "residents = agent_type_filter(train_data)\n",
    "train_data = train_data[train_data['agent'].isin(residents)]\n",
    "\n",
    "test_data = test_data[test_data['agent'].isin(residents)]\n",
    "\n",
    "train_data['duration'] = (pd.to_datetime(train_data['finished_at']) - pd.to_datetime(train_data['started_at'])).dt.total_seconds() / 60\n",
    "test_data['duration'] = (pd.to_datetime(test_data['finished_at']) - pd.to_datetime(test_data['started_at'])).dt.total_seconds() / 60\n",
    "\n",
    "train_data = train_data[train_data['duration'] > 15]\n",
    "test_data = test_data[test_data['duration'] > 15]\n",
    "\n",
    "train_map_df = (\n",
    "    train_data[['latitude', 'longitude', 'location_id']]\n",
    "    .dropna(subset=['latitude', 'longitude', 'location_id'])\n",
    "    .groupby(['latitude', 'longitude'])['location_id']\n",
    "    .agg(lambda s: s.value_counts().idxmax())   # MODE location_id per (lat,lon)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "train_lookup = dict(zip(zip(train_map_df.latitude, train_map_df.longitude),\n",
    "                        train_map_df.location_id))\n",
    "\n",
    "test_coords = list(zip(test_data['latitude'], test_data['longitude']))\n",
    "test_data['mapped_id'] = [train_lookup.get(k) for k in test_coords]\n",
    "\n",
    "next_id = int(train_data['location_id'].max()) + 1\n",
    "\n",
    "new_coords = (\n",
    "    test_data.loc[pd.isna(test_data['mapped_id']), ['latitude', 'longitude']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "new_coords['new_id'] = range(next_id, next_id + len(new_coords))\n",
    "\n",
    "new_lookup = dict(zip(zip(new_coords.latitude, new_coords.longitude),\n",
    "                      new_coords.new_id))\n",
    "\n",
    "test_data['location_id'] = [\n",
    "    mid if not pd.isna(mid) else new_lookup[(lat, lon)]\n",
    "    for (lat, lon), mid in zip(test_coords, test_data['mapped_id'])\n",
    "]\n",
    "\n",
    "test_data = test_data.drop(columns=['mapped_id'])\n",
    "test_data['location_id'] = test_data['location_id'].astype(int)\n",
    "\n",
    "train_data.rename(columns={'category': 'poi_category'}, inplace=True)\n",
    "test_data.rename(columns={'category': 'poi_category'}, inplace=True)\n",
    "\n",
    "train_data.drop(columns= ['distance_meters', 'duration', 'poi_id'], inplace=True)\n",
    "test_data.drop(columns= ['distance_meters', 'duration', 'poi_id'], inplace=True)\n",
    "\n",
    "train_data.to_csv('../processed/trial5/stop_past/agent_bucket=0.parquet', index=False)\n",
    "test_data.to_csv('../processed/trial5/stop_future/agent_bucket=0.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "458715ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent</th>\n",
       "      <th>started_at</th>\n",
       "      <th>finished_at</th>\n",
       "      <th>location_id</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>poi_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-31 15:00:00+00:00</td>\n",
       "      <td>2025-01-31 23:16:40+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>139.456186</td>\n",
       "      <td>35.697589</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-31 23:22:00+00:00</td>\n",
       "      <td>2025-01-31 23:38:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>139.441200</td>\n",
       "      <td>35.687694</td>\n",
       "      <td>workplace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-31 23:49:50+00:00</td>\n",
       "      <td>2025-02-02 00:17:10+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>139.456186</td>\n",
       "      <td>35.697589</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-02 00:24:30+00:00</td>\n",
       "      <td>2025-02-02 04:06:10+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>139.481339</td>\n",
       "      <td>35.701120</td>\n",
       "      <td>restaurant:workplace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-02 04:18:10+00:00</td>\n",
       "      <td>2025-02-02 04:55:40+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>139.457333</td>\n",
       "      <td>35.713718</td>\n",
       "      <td>restaurant:workplace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agent                started_at               finished_at  location_id  \\\n",
       "0      0 2025-01-31 15:00:00+00:00 2025-01-31 23:16:40+00:00            0   \n",
       "1      0 2025-01-31 23:22:00+00:00 2025-01-31 23:38:00+00:00            1   \n",
       "3      0 2025-01-31 23:49:50+00:00 2025-02-02 00:17:10+00:00            0   \n",
       "4      0 2025-02-02 00:24:30+00:00 2025-02-02 04:06:10+00:00            3   \n",
       "6      0 2025-02-02 04:18:10+00:00 2025-02-02 04:55:40+00:00            5   \n",
       "\n",
       "    longitude   latitude          poi_category  \n",
       "0  139.456186  35.697589           residential  \n",
       "1  139.441200  35.687694             workplace  \n",
       "3  139.456186  35.697589           residential  \n",
       "4  139.481339  35.701120  restaurant:workplace  \n",
       "6  139.457333  35.713718  restaurant:workplace  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data['agent'] == 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee836f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent</th>\n",
       "      <th>started_at</th>\n",
       "      <th>finished_at</th>\n",
       "      <th>location_id</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>poi_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81920</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-31 15:00:00+00:00</td>\n",
       "      <td>2025-04-01 01:46:50+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>139.456186</td>\n",
       "      <td>35.697589</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81922</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-01 01:59:30+00:00</td>\n",
       "      <td>2025-04-01 09:35:10+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>139.481339</td>\n",
       "      <td>35.701120</td>\n",
       "      <td>restaurant:workplace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81924</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-01 09:47:00+00:00</td>\n",
       "      <td>2025-04-02 01:53:10+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>139.456186</td>\n",
       "      <td>35.697589</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81925</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-02 02:00:50+00:00</td>\n",
       "      <td>2025-04-02 10:47:10+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>139.481339</td>\n",
       "      <td>35.701120</td>\n",
       "      <td>restaurant:workplace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81926</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-02 10:55:50+00:00</td>\n",
       "      <td>2025-04-02 11:47:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>139.481339</td>\n",
       "      <td>35.701120</td>\n",
       "      <td>restaurant:workplace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       agent                started_at               finished_at  location_id  \\\n",
       "81920      0 2025-03-31 15:00:00+00:00 2025-04-01 01:46:50+00:00            0   \n",
       "81922      0 2025-04-01 01:59:30+00:00 2025-04-01 09:35:10+00:00            3   \n",
       "81924      0 2025-04-01 09:47:00+00:00 2025-04-02 01:53:10+00:00            0   \n",
       "81925      0 2025-04-02 02:00:50+00:00 2025-04-02 10:47:10+00:00            3   \n",
       "81926      0 2025-04-02 10:55:50+00:00 2025-04-02 11:47:00+00:00            3   \n",
       "\n",
       "        longitude   latitude          poi_category  \n",
       "81920  139.456186  35.697589           residential  \n",
       "81922  139.481339  35.701120  restaurant:workplace  \n",
       "81924  139.456186  35.697589           residential  \n",
       "81925  139.481339  35.701120  restaurant:workplace  \n",
       "81926  139.481339  35.701120  restaurant:workplace  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[test_data['agent'] == 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ad61f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rename(columns={'agent_id': 'agent', 'start_datetime': 'started_at', 'end_datetime': 'finished_at', 'latitude_sp': 'latitude', 'longitude_sp': 'longitude', 'poi_id': 'location_id', 'category': 'poi_category'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0886dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(columns=['distance_m', 'duration'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aed10fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = merge_consecutive_locations(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9deac6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/sim2_evalb/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(test)\n",
    "\n",
    "eval_b_train = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_stay_points_test_anomalous.parquet')\n",
    "poi_data = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_poi.parquet')\n",
    "eval_b_train = eval_b_train.merge(poi_data, on='poi_id', how='left')\n",
    "eval_b_train\n",
    "\n",
    "eval_b_train.rename(columns={'agent_id': 'agent', 'start_datetime':'started_at', 'end_datetime':'finished_at', 'poi_id': 'location_id', 'category':'poi_category'}, inplace=True)\n",
    "eval_b_train.drop(columns=['source', 'anomaly_type', 'anomaly'], inplace=True)\n",
    "\n",
    "gt = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail_4_sim1_fis/anomalous_temporal_sim1.csv')\n",
    "ground_truth = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail_4_sim1_fis/anomalous_agent_sim1.csv')\n",
    "\n",
    "gt\n",
    "\n",
    "eval_b_train = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_stay_points_test_anomalous.parquet')\n",
    "eval_b_train = eval_b_train[eval_b_train['anomaly'] == True]\n",
    "eval_b_train.rename(columns={'agent_id': 'agent', 'start_datetime': 'anomaly_start_time', 'end_datetime': 'anomaly_end_time'}, inplace=True)\n",
    "eval_b_train.to_csv('../processed/anomalous_temporal.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9be9c",
   "metadata": {},
   "source": [
    "### Ground Truth Processing\n",
    "\n",
    "For the evaluation purpose, you need to have a anomalous_agents.csv saved in processed folder,\n",
    "the file ideally should contain a single column called 'agent', which contain the ids of all the anomalous agents in the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b1d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail5/gt_temporal/anomalous_temporal (1).csv')\n",
    "data2 = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail5/gt_temporal/anomalous_temporal (2).csv')\n",
    "data3 = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail5/gt_temporal/anomalous_temporal (3).csv')\n",
    "data4 = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail5/gt_temporal/anomalous_temporal (4).csv')\n",
    "\n",
    "gt = pd.concat([data1, data2, data3, data4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.rename(columns={'event_start_time': 'started_at', 'event_end_time': 'finished_at'}, inplace=True)\n",
    "gt = gt[['agent', 'started_at', 'finished_at']].copy()\n",
    "gt['started_at'] = pd.to_datetime(gt['started_at'])\n",
    "gt['finished_at'] = pd.to_datetime(gt['finished_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1684bef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.agent.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('/Users/chanuka/Desktop/codespaces/neural_reeb/data/trail_4_sim1_fis/anomalous_temporal_sim1.csv')\n",
    "gt_new = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_stay_points_test_anomalous.parquet')\n",
    "gt_new['start_datetime'] = pd.to_datetime(gt_new['start_datetime'], utc=True).dt.tz_convert('Asia/Tokyo')\n",
    "gt_new['end_datetime'] = pd.to_datetime(gt_new['end_datetime'], utc=True).dt.tz_convert('Asia/Tokyo')\n",
    "gt_new.rename(columns={'agent_id': 'agent', 'start_datetime': 'started_at', 'end_datetime': 'finished_at'}, inplace=True) # should have agent, started_at, finished_at\n",
    "gt_new = gt_new[gt_new['anomaly']== True]\n",
    "gt_new = gt_new[['agent', 'started_at', 'finished_at']]\n",
    "\n",
    "gt_new_agents = pd.DataFrame(gt_new.agent.unique(), columns=['agent'])\n",
    "\n",
    "# format 1\n",
    "gt_new_agents.to_csv('../processed/anomalous_agents.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e621b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = split_by_time_bins(gt)\n",
    "\n",
    "gt['time_segment'] = gt['started_at'].apply(assign_time_segment)\n",
    "gt['day_of_week'] = gt['started_at'].dt.dayofweek\n",
    "gt['day_type'] = gt['day_of_week'].apply(lambda x: 'weekend' if x >= 5 else 'weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d284b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.to_csv('../processed/trial5/anomalous_temporal.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
