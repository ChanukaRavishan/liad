{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332ac2df",
   "metadata": {},
   "source": [
    "### Processing LIAD-Framework ready data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e519da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from joblib import Parallel, delayed\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e61c1d",
   "metadata": {},
   "source": [
    "### Train / Test data Processing\n",
    "\n",
    "Train and Test data should contain:\n",
    "\n",
    "1. \"agent\"        : Id of the agent\n",
    "\n",
    "2. \"started_at\".  : Date and time. We assume the data you provide are UTC, then we convert to Asia/Tokyo.\n",
    "\n",
    "3. \"finished_at\"  : Date and time. We assume the data you provide are UTC, then we convert to Asia/Tokyo.\n",
    "\n",
    "4. \"location_id\"  : Id of the location\n",
    "\n",
    "5. \"latitude\"       \n",
    "\n",
    "6. \"longitude\"\n",
    "\n",
    "7. \"poi_category\" : POI label of the location\n",
    "\n",
    "Store the data as train.csv and test.csv in the processed folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f095c7",
   "metadata": {},
   "source": [
    "### Ground Truth Processing\n",
    "\n",
    "For the evaluation purpose, you need to have a anomalous_agents.csv saved in processed folder,\n",
    "the file ideally should contain a single column called 'agent', which contain the ids of all the anomalous agents in the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3137e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some of our data processing steps may or may not be important for you\n",
    "\n",
    "train = '/Users/chanuka/Desktop/codespaces/neural_reeb/data/trail_4_sim1_fis/sim1_fis_sp_past_trajectories_with_POI.csv'\n",
    "test = '/Users/chanuka/Desktop/codespaces/neural_reeb/data/trail_4_sim1_fis/sim1_fis_sp_future_trajectories_with_POI.csv'\n",
    "\n",
    "train_data = pd.read_csv(train)\n",
    "test_data = pd.read_csv(test)\n",
    "\n",
    "eval_b_train = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_stay_points_test_anomalous.parquet')\n",
    "poi_data = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_poi.parquet')\n",
    "eval_b_train = eval_b_train.merge(poi_data, on='poi_id', how='left')\n",
    "eval_b_train\n",
    "\n",
    "eval_b_train.rename(columns={'agent_id': 'agent', 'start_datetime':'started_at', 'end_datetime':'finished_at', 'poi_id': 'location_id', 'category':'poi_category'}, inplace=True)\n",
    "eval_b_train.drop(columns=['source', 'anomaly_type', 'anomaly'], inplace=True)\n",
    "\n",
    "gt = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail_4_sim1_fis/anomalous_temporal_sim1.csv')\n",
    "ground_truth = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail_4_sim1_fis/anomalous_agent_sim1.csv')\n",
    "\n",
    "gt\n",
    "\n",
    "eval_b_train = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_stay_points_test_anomalous.parquet')\n",
    "eval_b_train = eval_b_train[eval_b_train['anomaly'] == True]\n",
    "eval_b_train.rename(columns={'agent_id': 'agent', 'start_datetime': 'anomaly_start_time', 'end_datetime': 'anomaly_end_time'}, inplace=True)\n",
    "eval_b_train.to_csv('../processed/anomalous_temporal.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9be9c",
   "metadata": {},
   "source": [
    "### GT processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('/Users/chanuka/Desktop/codespaces/neural_reeb/data/trail_4_sim1_fis/anomalous_temporal_sim1.csv')\n",
    "gt_new = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_stay_points_test_anomalous.parquet')\n",
    "gt_new['start_datetime'] = pd.to_datetime(gt_new['start_datetime'], utc=True).dt.tz_convert('Asia/Tokyo')\n",
    "gt_new['end_datetime'] = pd.to_datetime(gt_new['end_datetime'], utc=True).dt.tz_convert('Asia/Tokyo')\n",
    "gt_new.rename(columns={'agent_id': 'agent', 'start_datetime': 'started_at', 'end_datetime': 'finished_at'}, inplace=True) # should have agent, started_at, finished_at\n",
    "gt_new = gt_new[gt_new['anomaly']== True]\n",
    "gt_new = gt_new[['agent', 'started_at', 'finished_at']]\n",
    "\n",
    "gt_new_agents = pd.DataFrame(gt_new.agent.unique(), columns=['agent'])\n",
    "\n",
    "# format 1\n",
    "gt_new_agents.to_csv('../processed/anomalous_agents.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e621b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_new = split_by_time_bins(gt_new)\n",
    "\n",
    "gt_new['time_segment'] = gt_new['started_at'].apply(assign_time_segment)\n",
    "gt_new['day_of_week'] = gt_new['started_at'].dt.dayofweek\n",
    "gt_new['day_type'] = gt_new['day_of_week'].apply(lambda x: 'weekend' if x >= 5 else 'weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75d284b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_new.to_csv('../processed/anomalous_segmented.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
