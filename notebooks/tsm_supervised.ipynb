{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c72063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "def parse_maybe_list(x):\n",
    "    \"\"\"\n",
    "    Safely parse list-like values that may be stored as strings in CSV.\n",
    "    Returns a Python list.\n",
    "    Handles: NaN/None, list, tuple, set, np.ndarray, strings like \"[1,2]\".\n",
    "    \"\"\"\n",
    "    # Fast path for common nulls\n",
    "    if x is None:\n",
    "        return []\n",
    "\n",
    "    # If it's already list-like, return it as a list\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, (set, tuple)):\n",
    "        return list(x)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "\n",
    "    # Handle scalar NaN (only safe for scalars)\n",
    "    if isinstance(x, (float, np.floating)) and np.isnan(x):\n",
    "        return []\n",
    "\n",
    "    # Strings: try literal_eval\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s == \"\" or s.lower() in (\"nan\", \"none\", \"null\"):\n",
    "            return []\n",
    "        try:\n",
    "            v = ast.literal_eval(s)\n",
    "            if isinstance(v, (list, set, tuple)):\n",
    "                return list(v)\n",
    "            if isinstance(v, np.ndarray):\n",
    "                return v.tolist()\n",
    "            if isinstance(v, dict):\n",
    "                # choose keys (adjust if you want values instead)\n",
    "                return list(v.keys())\n",
    "            return []\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    # For anything else, try a safe pandas scalar-null check\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return []\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "\n",
    "def ensure_columns_exist(df, cols, fill_value=np.nan):\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = fill_value\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_anomaly_features(train_profiles: pd.DataFrame, test_profiles: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build per-row anomaly features by comparing test slot vs train slot\n",
    "    for the SAME agent + day_type + time_segment.\n",
    "\n",
    "    Prevents many-to-many merges by aggregating to unique keys first.\n",
    "    \"\"\"\n",
    "    keys = ['agent', 'day_type', 'time_segment']\n",
    "\n",
    "    base_numeric = [\n",
    "        'unique_location_ids',\n",
    "        'avg_distance_from_home_km',\n",
    "        'avg_speed_kmh',\n",
    "        'max_stay_duration',\n",
    "        'transformations',\n",
    "        'max_distance_from_home',\n",
    "        'label'\n",
    "    ]\n",
    "    base_misc = ['unique_locs', 'poi_dict', 'dominent_poi']\n",
    "\n",
    "    train_profiles = ensure_columns_exist(train_profiles.copy(), base_numeric + base_misc)\n",
    "    test_profiles  = ensure_columns_exist(test_profiles.copy(),  base_numeric + base_misc)\n",
    "\n",
    "    # Aggregate to ensure 1 row per key in each split\n",
    "    agg_num = {\n",
    "        'unique_location_ids': 'mean',\n",
    "        'avg_distance_from_home_km': 'mean',\n",
    "        'avg_speed_kmh': 'mean',\n",
    "        'max_stay_duration': 'max',\n",
    "        'transformations': 'mean',\n",
    "        'max_distance_from_home': 'max',\n",
    "        'label': 'max'\n",
    "    }\n",
    "    # For list-like / categorical fields: take first non-null occurrence\n",
    "    agg_misc = {\n",
    "        'unique_locs': 'first',\n",
    "        'poi_dict': 'first',\n",
    "        'dominent_poi': 'first'\n",
    "    }\n",
    "\n",
    "    train_agg = train_profiles.groupby(keys, as_index=False).agg({**agg_num, **agg_misc})\n",
    "    test_agg  = test_profiles.groupby(keys, as_index=False).agg({**agg_num, **agg_misc})\n",
    "\n",
    "    # Merge on agent+slot (THE actual intended join)\n",
    "    merged = pd.merge(\n",
    "        test_agg,\n",
    "        train_agg,\n",
    "        on=keys,\n",
    "        suffixes=('_test', '_train'),\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    numeric_train_cols = [\n",
    "        'unique_location_ids_train',\n",
    "        'avg_distance_from_home_km_train',\n",
    "        'avg_speed_kmh_train',\n",
    "        'max_stay_duration_train',\n",
    "        'transformations_train',\n",
    "        'max_distance_from_home_train'\n",
    "    ]\n",
    "    for c in numeric_train_cols:\n",
    "        if c not in merged.columns:\n",
    "            merged[c] = 0.0\n",
    "    merged[numeric_train_cols] = merged[numeric_train_cols].fillna(0)\n",
    "\n",
    "\n",
    "    for c in ['unique_locs_train', 'unique_locs_test', 'poi_dict_train', 'poi_dict_test']:\n",
    "        if c not in merged.columns:\n",
    "            merged[c] = [[]] * len(merged)\n",
    "        merged[c] = merged[c].apply(parse_maybe_list)\n",
    "\n",
    "    merged['f_count_diff'] = (merged['unique_location_ids_test'] - merged['unique_location_ids_train']).abs()\n",
    "    merged['f_dist_diff']  = (merged['avg_distance_from_home_km_test'] - merged['avg_distance_from_home_km_train']).abs()\n",
    "    merged['f_speed_diff'] = (merged['avg_speed_kmh_test'] - merged['avg_speed_kmh_train']).abs()\n",
    "\n",
    "    def get_new_loc_count(row):\n",
    "        set_train = set(row['unique_locs_train']) if isinstance(row['unique_locs_train'], list) else set()\n",
    "        set_test  = set(row['unique_locs_test'])  if isinstance(row['unique_locs_test'], list)  else set()\n",
    "        return len(set_test - set_train)\n",
    "\n",
    "    merged['f_new_locs'] = merged.apply(get_new_loc_count, axis=1)\n",
    "\n",
    "    merged['f_max_stay_diff'] = (merged['max_stay_duration_test'] - merged['max_stay_duration_train']).abs()\n",
    "    merged['f_transforms_diff'] = (merged['transformations_test'] - merged['transformations_train']).abs()\n",
    "    merged['f_max_dist_diff'] = (merged['max_distance_from_home_test'] - merged['max_distance_from_home_train']).abs()\n",
    "\n",
    "    if 'dominent_poi_test' not in merged.columns:\n",
    "        merged['dominent_poi_test'] = np.nan\n",
    "    if 'dominent_poi_train' not in merged.columns:\n",
    "        merged['dominent_poi_train'] = np.nan\n",
    "\n",
    "    merged['f_dom_poi_changed'] = (merged['dominent_poi_test'] != merged['dominent_poi_train']).astype(int)\n",
    "\n",
    "    def get_new_poi_count(row):\n",
    "        set_train = set(row['poi_dict_train']) if isinstance(row['poi_dict_train'], list) else set()\n",
    "        set_test  = set(row['poi_dict_test'])  if isinstance(row['poi_dict_test'], list)  else set()\n",
    "        return len(set_test - set_train)\n",
    "\n",
    "    merged['f_new_pois'] = merged.apply(get_new_poi_count, axis=1)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def fit_anomaly_weight_model(train_profiles: pd.DataFrame, test_profiles: pd.DataFrame):\n",
    "    merged = build_anomaly_features(train_profiles, test_profiles)\n",
    "\n",
    "    feature_cols = [\n",
    "        'f_count_diff',\n",
    "        'f_dist_diff',\n",
    "        'f_speed_diff',\n",
    "        'f_new_locs',\n",
    "        'f_max_stay_diff',\n",
    "        'f_transforms_diff',\n",
    "        'f_max_dist_diff',\n",
    "        'f_dom_poi_changed',\n",
    "        'f_new_pois',\n",
    "    ]\n",
    "\n",
    "    X = merged[feature_cols]\n",
    "    y = merged['label_test']  # 0/1 anomalous row in test\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(\n",
    "            class_weight='balanced',\n",
    "            max_iter=1000,\n",
    "            n_jobs=1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    model.fit(X, y)\n",
    "    return model, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54fdb7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT agents: 274\n",
      "Available normal agents: 371183\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/processed/sim2_evalb/train_monthly.csv')\n",
    "test  = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/processed/sim2_evalb/test_monthly.csv')\n",
    "\n",
    "gt = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/processed/sim2_evalb/anomalous_segmented.csv')\n",
    "residents = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/processed/sim2_evalb/residents.csv')\n",
    "\n",
    "train = train[train['agent'].isin(residents['agent'].unique())].copy()\n",
    "\n",
    "train['label'] = 0\n",
    "test['label'] = 0\n",
    "\n",
    "gt_agents = set(gt['agent'].unique())\n",
    "train_agents = set(train['agent'].unique())\n",
    "normal_agents = np.array(list(train_agents - gt_agents))\n",
    "\n",
    "print(\"GT agents:\", len(gt_agents))\n",
    "print(\"Available normal agents:\", len(normal_agents))\n",
    "\n",
    "np.random.seed(42)\n",
    "sample_size = 100\n",
    "if len(normal_agents) < sample_size:\n",
    "    raise ValueError(f\"Not enough normal agents to sample {sample_size}. Only {len(normal_agents)} available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92fa051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_normals = np.random.choice(normal_agents, size=sample_size, replace=False)\n",
    "\n",
    "train = pd.concat([\n",
    "    train[train['agent'].isin(gt_agents)],\n",
    "    train[train['agent'].isin(sampled_normals)]\n",
    "], ignore_index=True)\n",
    "\n",
    "test = test[test['agent'].isin(train['agent'].unique())].copy()\n",
    "\n",
    "gt_keys = set(zip(gt['agent'], gt['day_type'], gt['time_segment']))\n",
    "test_keys = list(zip(test['agent'], test['day_type'], test['time_segment']))\n",
    "test['label'] = np.fromiter((k in gt_keys for k in test_keys), dtype=np.int8, count=len(test))\n",
    "\n",
    "for col in ['unique_locs', 'poi_dict']:\n",
    "    if col in train.columns:\n",
    "        train[col] = train[col].apply(parse_maybe_list)\n",
    "    if col in test.columns:\n",
    "        test[col] = test[col].apply(parse_maybe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a849612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting the model\n",
      "f_count_diff 0.15302082531276692\n",
      "f_dist_diff -0.1913591142485288\n",
      "f_speed_diff 0.24952645751043032\n",
      "f_new_locs -0.5090649992846829\n",
      "f_max_stay_diff 0.0780266674132837\n",
      "f_transforms_diff -0.03548937193854445\n",
      "f_max_dist_diff 0.4501840772380571\n",
      "f_dom_poi_changed 0.029679350593119064\n",
      "f_new_pois 0.22113424571069545\n",
      "\n",
      "Saved weights to: sim2_evalb_model_weights.csv\n"
     ]
    }
   ],
   "source": [
    "print('fitting the model')\n",
    "model, feature_cols = fit_anomaly_weight_model(train, test)\n",
    "\n",
    "clf = model.named_steps['clf']\n",
    "weights = clf.coef_[0]\n",
    "\n",
    "# print weights\n",
    "for name, w in zip(feature_cols, weights):\n",
    "    print(name, w)\n",
    "\n",
    "# save weights\n",
    "weights_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"weight\": weights\n",
    "}).sort_values(\"weight\", key=abs, ascending=False)\n",
    "\n",
    "out_path = \"sim2_evalb_model_weights.csv\"\n",
    "weights_df.to_csv(out_path, index=False)\n",
    "print(f\"\\nSaved weights to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daef8713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "NUM_COLS = [\n",
    "    \"unique_location_ids\",\n",
    "    \"avg_distance_from_home_km\",\n",
    "    \"avg_speed_kmh\",\n",
    "    \"max_stay_duration\",\n",
    "    \"transformations\",\n",
    "    \"max_distance_from_home\",\n",
    "]\n",
    "\n",
    "def _safe_set(x):\n",
    "    if isinstance(x, list):\n",
    "        return set(x)\n",
    "    if isinstance(x, (set, tuple)):\n",
    "        return set(x)\n",
    "    return set()\n",
    "\n",
    "def build_weekly_features(weekly_profiles: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = weekly_profiles.copy()\n",
    "\n",
    "    # --- Sort for lag features ---\n",
    "    df = df.sort_values([\"agent\", \"day_type\", \"time_segment\", \"week_id\"])\n",
    "\n",
    "    # --- Training stats per slot (level baseline) ---\n",
    "    train = df[df[\"phase\"] == \"train\"].copy()\n",
    "\n",
    "    level_mu = (\n",
    "        train.groupby([\"agent\", \"day_type\", \"time_segment\"])[NUM_COLS]\n",
    "        .mean()\n",
    "        .add_suffix(\"_mu_train\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    level_sd = (\n",
    "        train.groupby([\"agent\", \"day_type\", \"time_segment\"])[NUM_COLS]\n",
    "        .std(ddof=0)\n",
    "        .fillna(0)\n",
    "        .add_suffix(\"_sd_train\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- Training deltas per slot (drift baseline) ---\n",
    "    # delta = this_week - prev_week within training weeks\n",
    "    train_d = train.sort_values([\"agent\", \"day_type\", \"time_segment\", \"week_id\"]).copy()\n",
    "    for c in NUM_COLS:\n",
    "        train_d[c + \"_delta\"] = train_d.groupby([\"agent\",\"day_type\",\"time_segment\"])[c].diff()\n",
    "\n",
    "    delta_cols = [c + \"_delta\" for c in NUM_COLS]\n",
    "    delta_mu = (\n",
    "        train_d.groupby([\"agent\", \"day_type\", \"time_segment\"])[delta_cols]\n",
    "        .mean()\n",
    "        .add_suffix(\"_mu_train\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    delta_sd = (\n",
    "        train_d.groupby([\"agent\", \"day_type\", \"time_segment\"])[delta_cols]\n",
    "        .std(ddof=0)\n",
    "        .fillna(0)\n",
    "        .add_suffix(\"_sd_train\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- Merge baselines into all rows ---\n",
    "    out = df.merge(level_mu, on=[\"agent\",\"day_type\",\"time_segment\"], how=\"left\") \\\n",
    "            .merge(level_sd, on=[\"agent\",\"day_type\",\"time_segment\"], how=\"left\") \\\n",
    "            .merge(delta_mu, on=[\"agent\",\"day_type\",\"time_segment\"], how=\"left\") \\\n",
    "            .merge(delta_sd, on=[\"agent\",\"day_type\",\"time_segment\"], how=\"left\")\n",
    "\n",
    "    # Fill missing baselines (agents/slots with no train history)\n",
    "    out = out.fillna(0)\n",
    "\n",
    "    # --- Lag (previous week) from combined timeline (train+test) ---\n",
    "    for c in NUM_COLS:\n",
    "        out[c + \"_prev\"] = out.groupby([\"agent\",\"day_type\",\"time_segment\"])[c].shift(1)\n",
    "        out[c + \"_delta_now\"] = out[c] - out[c + \"_prev\"]\n",
    "\n",
    "    # --- Feature construction: level z + drift z ---\n",
    "    feature_cols = []\n",
    "    for c in NUM_COLS:\n",
    "        mu = out[c + \"_mu_train\"]\n",
    "        sd = out[c + \"_sd_train\"]\n",
    "        out[f\"f_{c}_level_z\"] = (out[c] - mu) / (sd + EPS)\n",
    "        feature_cols.append(f\"f_{c}_level_z\")\n",
    "\n",
    "        dmu = out[c + \"_delta_mu_train\"]\n",
    "        dsd = out[c + \"_delta_sd_train\"]\n",
    "        out[f\"f_{c}_drift_z\"] = (out[c + \"_delta_now\"] - dmu) / (dsd + EPS)\n",
    "        feature_cols.append(f\"f_{c}_drift_z\")\n",
    "\n",
    "        out[f\"f_{c}_abs_level\"] = (out[c] - mu).abs()\n",
    "        out[f\"f_{c}_abs_drift\"] = (out[c + \"_delta_now\"] - dmu).abs()\n",
    "        feature_cols += [f\"f_{c}_abs_level\", f\"f_{c}_abs_drift\"]\n",
    "\n",
    "    # --- Set novelty features ---\n",
    "    # Union sets over training for each agent (global baseline)\n",
    "    train_locs_union = train.groupby(\"agent\")[\"unique_locs\"].apply(\n",
    "        lambda s: set().union(*[_safe_set(x) for x in s])\n",
    "    )\n",
    "    train_pois_union = train.groupby(\"agent\")[\"poi_dict\"].apply(\n",
    "        lambda s: set().union(*[_safe_set(x) for x in s])\n",
    "    )\n",
    "\n",
    "    def new_count(row, union_series, col):\n",
    "        base = union_series.get(row[\"agent\"], set())\n",
    "        cur = _safe_set(row.get(col))\n",
    "        return len(cur - base)\n",
    "\n",
    "    out[\"f_new_locs_vs_train\"] = out.apply(lambda r: new_count(r, train_locs_union, \"unique_locs\"), axis=1)\n",
    "    out[\"f_new_pois_vs_train\"] = out.apply(lambda r: new_count(r, train_pois_union, \"poi_dict\"), axis=1)\n",
    "    feature_cols += [\"f_new_locs_vs_train\", \"f_new_pois_vs_train\"]\n",
    "\n",
    "    # Optional: dominant POI change vs train mode (if you have it reliably)\n",
    "    if \"dominant_poi\" in df.columns:\n",
    "        poi_mode = train.groupby([\"agent\",\"day_type\",\"time_segment\"])[\"dominant_poi\"] \\\n",
    "                        .agg(lambda x: x.mode().iloc[0] if len(x.mode()) else None) \\\n",
    "                        .reset_index() \\\n",
    "                        .rename(columns={\"dominant_poi\": \"dominant_poi_mode_train\"})\n",
    "        out = out.merge(poi_mode, on=[\"agent\",\"day_type\",\"time_segment\"], how=\"left\")\n",
    "        out[\"f_dom_poi_changed\"] = (out[\"dominant_poi\"] != out[\"dominant_poi_mode_train\"]).astype(int)\n",
    "        feature_cols.append(\"f_dom_poi_changed\")\n",
    "\n",
    "    out.attrs[\"feature_cols\"] = feature_cols\n",
    "    return out\n",
    "\n",
    "def fit_weekly_row_model(features_df: pd.DataFrame, label_col=\"label\"):\n",
    "    feature_cols = features_df.attrs[\"feature_cols\"]\n",
    "    train_rows = features_df[features_df[\"phase\"].isin([\"train\",\"test\"])].copy()\n",
    "\n",
    "    # IMPORTANT: train on TRAIN+TEST? Usually you train on train+some validation.\n",
    "    # Here we'll fit on whatever rows have labels.\n",
    "    train_rows = train_rows[train_rows[label_col].notna()].copy()\n",
    "\n",
    "    X = train_rows[feature_cols].astype(float)\n",
    "    y = train_rows[label_col].astype(int)\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=2000))\n",
    "    ])\n",
    "    model.fit(X, y)\n",
    "    return model, feature_cols\n",
    "\n",
    "def score_rows(model, features_df: pd.DataFrame, feature_cols):\n",
    "    X = features_df[feature_cols].astype(float)\n",
    "    # probability of anomaly\n",
    "    p = model.predict_proba(X)[:, 1]\n",
    "    out = features_df.copy()\n",
    "    out[\"anomaly_prob\"] = p\n",
    "    return out\n",
    "\n",
    "def pool_week_score(scored_rows: pd.DataFrame, k=10):\n",
    "    # top-k mean pooling per agent-week\n",
    "    def topk_mean(x):\n",
    "        x = np.sort(x)[::-1]\n",
    "        return float(np.mean(x[:min(k, len(x))])) if len(x) else 0.0\n",
    "\n",
    "    return scored_rows.groupby([\"agent\",\"week_id\"], as_index=False)[\"anomaly_prob\"].agg(\n",
    "        week_score=topk_mean,\n",
    "        max_score=\"max\",\n",
    "        mean_score=\"mean\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeaa0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 'chunk' if 0 -> 5, 1-> 6, 2-> 7, 3-> 8,4-> 9\n",
    "test[\"chunk\"] = test[\"chunk\"].replace({0: 5, 1: 6, 2: 7, 3: 8, 4: 9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4974d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.rename(columns={'chunk': 'week_id'}, inplace=True)\n",
    "test['phase'] = 'test'\n",
    "train['phase'] = 'train'\n",
    "# append test dataframe to train at the end\n",
    "new_df = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb7ba467",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = build_weekly_features(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f5e74d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'feature_cols'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model, feature_cols = \u001b[43mfit_weekly_row_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mfit_weekly_row_model\u001b[39m\u001b[34m(features_df, label_col)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_weekly_row_model\u001b[39m(features_df: pd.DataFrame, label_col=\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     feature_cols = \u001b[43mfeatures_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature_cols\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    134\u001b[39m     train_rows = features_df[features_df[\u001b[33m\"\u001b[39m\u001b[33mphase\u001b[39m\u001b[33m\"\u001b[39m].isin([\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m])].copy()\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# IMPORTANT: train on TRAIN+TEST? Usually you train on train+some validation.\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# Here we'll fit on whatever rows have labels.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'feature_cols'"
     ]
    }
   ],
   "source": [
    "model, feature_cols = fit_weekly_row_model(new_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
