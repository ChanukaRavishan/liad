{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf10d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from joblib import Parallel, delayed\n",
    "import math\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b19550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training profiles: 6991244 entries for 583452 agents\n",
      "Testing profiles: 6667615 entries for 559366 agents\n",
      "\n",
      "Common agents: 559366\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/processed/sim2_evalb/train_monthly.csv')\n",
    "test = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/processed/sim2_evalb/test_monthly.csv')\n",
    "\n",
    "print(f\"Training profiles: {len(train)} entries for {train['agent'].nunique()} agents\")\n",
    "print(f\"Testing profiles: {len(test)} entries for {test['agent'].nunique()} agents\")\n",
    "print(f\"\\nCommon agents: {len(set(train['agent'].unique()) & set(test['agent'].unique()))}\")\n",
    "\n",
    "gt = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/processed/sim2_evalb/anomalous_segmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b262dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = 0\n",
    "test['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f519ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT agents: 274\n",
      "Available normal agents: 583199\n"
     ]
    }
   ],
   "source": [
    "gt_agents = set(gt['agent'].unique())\n",
    "train_agents = set(train['agent'].unique())\n",
    "normal_agents = np.array(list(train_agents - gt_agents))\n",
    "\n",
    "print(\"GT agents:\", len(gt_agents))\n",
    "print(\"Available normal agents:\", len(normal_agents))\n",
    "np.random.seed(42)\n",
    "sampled_normals = np.random.choice(normal_agents, size=1000, replace=False)\n",
    "\n",
    "train = pd.concat([\n",
    "    train[train['agent'].isin(gt_agents)],          # anomalous agents\n",
    "    train[train['agent'].isin(sampled_normals)]     # clean agents\n",
    "]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a409b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test.agent.isin(train.agent.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983cf0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('temp.csv', index=False)\n",
    "# test.to_csv('temp_test.csv', index=False)\n",
    "\n",
    "train = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/notebooks/temp.csv')\n",
    "test = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/notebooks/temp_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "739a1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent, gt_agent in gt.groupby('agent'):\n",
    "    agent_mask = test['agent'] == agent\n",
    "\n",
    "    if not agent_mask.any():\n",
    "        continue\n",
    "\n",
    "    for _, row in gt_agent.iterrows():\n",
    "        anomaly_time_segment = row['time_segment']\n",
    "        anomaly_day_type = row['day_type']\n",
    "\n",
    "        overlap_mask = (\n",
    "            agent_mask &\n",
    "            (test['day_type'] == anomaly_day_type) &\n",
    "            (test['time_segment'] == anomaly_time_segment)\n",
    "        )\n",
    "\n",
    "        test.loc[overlap_mask, 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4780532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_anomaly_features(train_profiles, test_profiles):\n",
    "    # Merge train/test profiles (don't filter by agent here; keep everything)\n",
    "    merged = pd.merge(\n",
    "        test_profiles,\n",
    "        train_profiles,\n",
    "        on=['day_type', 'time_segment'],\n",
    "        suffixes=('_test', '_train'),\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill numeric training columns when no history exists\n",
    "    numeric_cols = [\n",
    "        'unique_location_ids_train',\n",
    "        'avg_distance_from_home_km_train',\n",
    "        'avg_speed_kmh_train',\n",
    "        'max_stay_duration_train',\n",
    "        'transformations_train',\n",
    "        'max_distance_from_home_train'\n",
    "    ]\n",
    "    merged[numeric_cols] = merged[numeric_cols].fillna(0)\n",
    "\n",
    "    # Component 1: Count difference\n",
    "    merged['f_count_diff'] = (merged['unique_location_ids_test'] -\n",
    "                              merged['unique_location_ids_train']).abs()\n",
    "\n",
    "    # Component 2: Distance difference\n",
    "    merged['f_dist_diff'] = (merged['avg_distance_from_home_km_test'] -\n",
    "                             merged['avg_distance_from_home_km_train']).abs()\n",
    "\n",
    "    # Component 3: Speed difference\n",
    "    merged['f_speed_diff'] = (merged['avg_speed_kmh_test'] -\n",
    "                              merged['avg_speed_kmh_train']).abs()\n",
    "\n",
    "    # Component 4: New locations\n",
    "    def get_new_loc_count(row):\n",
    "        locs_train = row['unique_locs_train']\n",
    "        locs_test = row['unique_locs_test']\n",
    "        set_train = set(locs_train) if isinstance(locs_train, list) else set()\n",
    "        set_test = set(locs_test) if isinstance(locs_test, list) else set()\n",
    "        return len(set_test - set_train)\n",
    "\n",
    "    merged['f_new_locs'] = merged.apply(get_new_loc_count, axis=1)\n",
    "\n",
    "    # Component 5: max stay duration\n",
    "    merged['f_max_stay_diff'] = (\n",
    "        merged['max_stay_duration_test'] -\n",
    "        merged['max_stay_duration_train']\n",
    "    ).abs()\n",
    "\n",
    "    # Component 6: number of transformations\n",
    "    merged['f_transforms_diff'] = (\n",
    "        merged['transformations_test'] -\n",
    "        merged['transformations_train']\n",
    "    ).abs()\n",
    "\n",
    "    # Component 7: max distance from home\n",
    "    merged['f_max_dist_diff'] = (\n",
    "        merged['max_distance_from_home_test'] -\n",
    "        merged['max_distance_from_home_train']\n",
    "    ).abs()\n",
    "\n",
    "    # Component 8: dominant poi changed\n",
    "    merged['f_dom_poi_changed'] = (\n",
    "        merged['dominent_poi_test'] != merged['dominent_poi_train']\n",
    "    ).astype(int)\n",
    "\n",
    "    # Component 9: new POI categories\n",
    "    def get_new_poi_count(row):\n",
    "        pois_train = row['poi_dict_train']\n",
    "        pois_test = row['poi_dict_test']\n",
    "        set_train = set(pois_train) if isinstance(pois_train, list) else set()\n",
    "        set_test = set(pois_test) if isinstance(pois_test, list) else set()\n",
    "        return len(set_test - set_train)\n",
    "\n",
    "    merged['f_new_pois'] = merged.apply(get_new_poi_count, axis=1)\n",
    "\n",
    "    return merged\n",
    "\n",
    "def fit_anomaly_weight_model(train_profiles, test_profiles):\n",
    "    merged = build_anomaly_features(train_profiles, test_profiles)\n",
    "\n",
    "    feature_cols = [\n",
    "        'f_count_diff',\n",
    "        'f_dist_diff',\n",
    "        'f_speed_diff',\n",
    "        'f_new_locs',\n",
    "        'f_max_stay_diff',\n",
    "        'f_transforms_diff',\n",
    "        'f_max_dist_diff',\n",
    "        'f_dom_poi_changed',\n",
    "        'f_new_pois',\n",
    "    ]\n",
    "\n",
    "    X = merged[feature_cols]\n",
    "    y = merged['label_test']  # 0/1 anomalous row\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(\n",
    "            class_weight='balanced',  # you likely have few anomalies\n",
    "            max_iter=1000\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    model.fit(X, y)\n",
    "    return model, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b27a210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def fit_anomaly_weight_model(train_profiles, test_profiles):\n",
    "    merged = build_anomaly_features(train_profiles, test_profiles)\n",
    "\n",
    "    feature_cols = [\n",
    "        'f_count_diff',\n",
    "        'f_dist_diff',\n",
    "        'f_speed_diff',\n",
    "        'f_new_locs',\n",
    "        'f_max_stay_diff',\n",
    "        'f_transforms_diff',\n",
    "        'f_max_dist_diff',\n",
    "        'f_dom_poi_changed',\n",
    "        'f_new_pois',\n",
    "    ]\n",
    "\n",
    "    X = merged[feature_cols]\n",
    "    y = merged['label_test']  # 0/1 anomalous row\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(\n",
    "            class_weight='balanced',  # you likely have few anomalies\n",
    "            max_iter=1000\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    model.fit(X, y)\n",
    "    return model, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be3740bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_count_diff 0.09441474976202284\n",
      "f_dist_diff -0.11272085849683175\n",
      "f_speed_diff 0.379662030687741\n",
      "f_new_locs 0.0\n",
      "f_max_stay_diff 0.18430967719595884\n",
      "f_transforms_diff -0.008703571242394305\n",
      "f_max_dist_diff 0.34405856529748496\n",
      "f_dom_poi_changed -0.001464858802034066\n",
      "f_new_pois 0.0\n"
     ]
    }
   ],
   "source": [
    "model, feature_cols = fit_anomaly_weight_model(train, test)\n",
    "scaler = model.named_steps['scaler']\n",
    "clf = model.named_steps['clf']\n",
    "\n",
    "weights = clf.coef_[0]\n",
    "for name, w in zip(feature_cols, weights):\n",
    "    print(name, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daef8713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "NUM_COLS = [\n",
    "    \"unique_location_ids\",\n",
    "    \"avg_distance_from_home_km\",\n",
    "    \"avg_speed_kmh\",\n",
    "    \"max_stay_duration\",\n",
    "    \"transformations\",\n",
    "    \"max_distance_from_home\",\n",
    "]\n",
    "\n",
    "def _safe_set(x):\n",
    "    if isinstance(x, list):\n",
    "        return set(x)\n",
    "    if isinstance(x, (set, tuple)):\n",
    "        return set(x)\n",
    "    return set()\n",
    "\n",
    "def build_weekly_features(weekly_profiles: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = weekly_profiles.copy()\n",
    "\n",
    "    # --- Sort for lag features ---\n",
    "    df = df.sort_values([\"agent\", \"day_type\", \"time_segment\", \"week_id\"])\n",
    "\n",
    "    # --- Training stats per slot (level baseline) ---\n",
    "    train = df[df[\"phase\"] == \"train\"].copy()\n",
    "\n",
    "    level_mu = (\n",
    "        train.groupby([\"agent\", \"day_type\", \"time_segment\"])[NUM_COLS]\n",
    "        .mean()\n",
    "        .add_suffix(\"_mu_train\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    level_sd = (\n",
    "        train.groupby([\"agent\", \"day_type\", \"time_segment\"])[NUM_COLS]\n",
    "        .std(ddof=0)\n",
    "        .fillna(0)\n",
    "        .add_suffix(\"_sd_train\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- Training deltas per slot (drift baseline) ---\n",
    "    # delta = this_week - prev_week within training weeks\n",
    "    train_d = train.sort_values([\"agent\", \"day_type\", \"time_segment\", \"week_id\"]).copy()\n",
    "    for c in NUM_COLS:\n",
    "        train_d[c + \"_delta\"] = train_d.groupby([\"agent\",\"day_type\",\"time_segment\"])[c].diff()\n",
    "\n",
    "    delta_cols = [c + \"_delta\" for c in NUM_COLS]\n",
    "    delta_mu = (\n",
    "        train_d.groupby([\"agent\", \"day_type\", \"time_segment\"])[delta_cols]\n",
    "        .mean()\n",
    "        .add_suffix(\"_mu_train\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    delta_sd = (\n",
    "        train_d.groupby([\"agent\", \"day_type\", \"time_segment\"])[delta_cols]\n",
    "        .std(ddof=0)\n",
    "        .fillna(0)\n",
    "        .add_suffix(\"_sd_train\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- Merge baselines into all rows ---\n",
    "    out = df.merge(level_mu, on=[\"agent\",\"day_type\",\"time_segment\"], how=\"left\") \\\n",
    "            .merge(level_sd, on=[\"agent\",\"day_type\",\"time_segment\"], how=\"left\") \\\n",
    "            .merge(delta_mu, on=[\"agent\",\"day_type\",\"time_segment\"], how=\"left\") \\\n",
    "            .merge(delta_sd, on=[\"agent\",\"day_type\",\"time_segment\"], how=\"left\")\n",
    "\n",
    "    # Fill missing baselines (agents/slots with no train history)\n",
    "    out = out.fillna(0)\n",
    "\n",
    "    # --- Lag (previous week) from combined timeline (train+test) ---\n",
    "    for c in NUM_COLS:\n",
    "        out[c + \"_prev\"] = out.groupby([\"agent\",\"day_type\",\"time_segment\"])[c].shift(1)\n",
    "        out[c + \"_delta_now\"] = out[c] - out[c + \"_prev\"]\n",
    "\n",
    "    # --- Feature construction: level z + drift z ---\n",
    "    feature_cols = []\n",
    "    for c in NUM_COLS:\n",
    "        mu = out[c + \"_mu_train\"]\n",
    "        sd = out[c + \"_sd_train\"]\n",
    "        out[f\"f_{c}_level_z\"] = (out[c] - mu) / (sd + EPS)\n",
    "        feature_cols.append(f\"f_{c}_level_z\")\n",
    "\n",
    "        dmu = out[c + \"_delta_mu_train\"]\n",
    "        dsd = out[c + \"_delta_sd_train\"]\n",
    "        out[f\"f_{c}_drift_z\"] = (out[c + \"_delta_now\"] - dmu) / (dsd + EPS)\n",
    "        feature_cols.append(f\"f_{c}_drift_z\")\n",
    "\n",
    "        out[f\"f_{c}_abs_level\"] = (out[c] - mu).abs()\n",
    "        out[f\"f_{c}_abs_drift\"] = (out[c + \"_delta_now\"] - dmu).abs()\n",
    "        feature_cols += [f\"f_{c}_abs_level\", f\"f_{c}_abs_drift\"]\n",
    "\n",
    "    # --- Set novelty features ---\n",
    "    # Union sets over training for each agent (global baseline)\n",
    "    train_locs_union = train.groupby(\"agent\")[\"unique_locs\"].apply(\n",
    "        lambda s: set().union(*[_safe_set(x) for x in s])\n",
    "    )\n",
    "    train_pois_union = train.groupby(\"agent\")[\"poi_dict\"].apply(\n",
    "        lambda s: set().union(*[_safe_set(x) for x in s])\n",
    "    )\n",
    "\n",
    "    def new_count(row, union_series, col):\n",
    "        base = union_series.get(row[\"agent\"], set())\n",
    "        cur = _safe_set(row.get(col))\n",
    "        return len(cur - base)\n",
    "\n",
    "    out[\"f_new_locs_vs_train\"] = out.apply(lambda r: new_count(r, train_locs_union, \"unique_locs\"), axis=1)\n",
    "    out[\"f_new_pois_vs_train\"] = out.apply(lambda r: new_count(r, train_pois_union, \"poi_dict\"), axis=1)\n",
    "    feature_cols += [\"f_new_locs_vs_train\", \"f_new_pois_vs_train\"]\n",
    "\n",
    "    # Optional: dominant POI change vs train mode (if you have it reliably)\n",
    "    if \"dominant_poi\" in df.columns:\n",
    "        poi_mode = train.groupby([\"agent\",\"day_type\",\"time_segment\"])[\"dominant_poi\"] \\\n",
    "                        .agg(lambda x: x.mode().iloc[0] if len(x.mode()) else None) \\\n",
    "                        .reset_index() \\\n",
    "                        .rename(columns={\"dominant_poi\": \"dominant_poi_mode_train\"})\n",
    "        out = out.merge(poi_mode, on=[\"agent\",\"day_type\",\"time_segment\"], how=\"left\")\n",
    "        out[\"f_dom_poi_changed\"] = (out[\"dominant_poi\"] != out[\"dominant_poi_mode_train\"]).astype(int)\n",
    "        feature_cols.append(\"f_dom_poi_changed\")\n",
    "\n",
    "    out.attrs[\"feature_cols\"] = feature_cols\n",
    "    return out\n",
    "\n",
    "def fit_weekly_row_model(features_df: pd.DataFrame, label_col=\"label\"):\n",
    "    feature_cols = features_df.attrs[\"feature_cols\"]\n",
    "    train_rows = features_df[features_df[\"phase\"].isin([\"train\",\"test\"])].copy()\n",
    "\n",
    "    # IMPORTANT: train on TRAIN+TEST? Usually you train on train+some validation.\n",
    "    # Here we'll fit on whatever rows have labels.\n",
    "    train_rows = train_rows[train_rows[label_col].notna()].copy()\n",
    "\n",
    "    X = train_rows[feature_cols].astype(float)\n",
    "    y = train_rows[label_col].astype(int)\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=2000))\n",
    "    ])\n",
    "    model.fit(X, y)\n",
    "    return model, feature_cols\n",
    "\n",
    "def score_rows(model, features_df: pd.DataFrame, feature_cols):\n",
    "    X = features_df[feature_cols].astype(float)\n",
    "    # probability of anomaly\n",
    "    p = model.predict_proba(X)[:, 1]\n",
    "    out = features_df.copy()\n",
    "    out[\"anomaly_prob\"] = p\n",
    "    return out\n",
    "\n",
    "def pool_week_score(scored_rows: pd.DataFrame, k=10):\n",
    "    # top-k mean pooling per agent-week\n",
    "    def topk_mean(x):\n",
    "        x = np.sort(x)[::-1]\n",
    "        return float(np.mean(x[:min(k, len(x))])) if len(x) else 0.0\n",
    "\n",
    "    return scored_rows.groupby([\"agent\",\"week_id\"], as_index=False)[\"anomaly_prob\"].agg(\n",
    "        week_score=topk_mean,\n",
    "        max_score=\"max\",\n",
    "        mean_score=\"mean\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeaa0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 'chunk' if 0 -> 5, 1-> 6, 2-> 7, 3-> 8,4-> 9\n",
    "test[\"chunk\"] = test[\"chunk\"].replace({0: 5, 1: 6, 2: 7, 3: 8, 4: 9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4974d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.rename(columns={'chunk': 'week_id'}, inplace=True)\n",
    "test['phase'] = 'test'\n",
    "train['phase'] = 'train'\n",
    "# append test dataframe to train at the end\n",
    "new_df = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb7ba467",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = build_weekly_features(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f5e74d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'feature_cols'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model, feature_cols = \u001b[43mfit_weekly_row_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mfit_weekly_row_model\u001b[39m\u001b[34m(features_df, label_col)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_weekly_row_model\u001b[39m(features_df: pd.DataFrame, label_col=\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     feature_cols = \u001b[43mfeatures_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature_cols\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    134\u001b[39m     train_rows = features_df[features_df[\u001b[33m\"\u001b[39m\u001b[33mphase\u001b[39m\u001b[33m\"\u001b[39m].isin([\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m])].copy()\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# IMPORTANT: train on TRAIN+TEST? Usually you train on train+some validation.\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# Here we'll fit on whatever rows have labels.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'feature_cols'"
     ]
    }
   ],
   "source": [
    "model, feature_cols = fit_weekly_row_model(new_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
