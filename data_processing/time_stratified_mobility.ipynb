{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332ac2df",
   "metadata": {},
   "source": [
    "### Time-Stratified Mobility Profiling (TSMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b22a15e",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from joblib import Parallel, delayed\n",
    "import math\n",
    "\n",
    "\n",
    "def agent_type_filter(train):\n",
    "\n",
    "    train_data = pd.read_csv(train)\n",
    "\n",
    "    train_data['started_at'] = pd.to_datetime(train_data['started_at'])\n",
    "    train_data['finished_at'] = pd.to_datetime(train_data['finished_at'])\n",
    "\n",
    "    train_data['duration_min'] = (train_data['finished_at'] - train_data['started_at']).dt.total_seconds() / 60.0\n",
    "    train_data['duration'] = train_data['duration_min'].clip(lower=0).fillna(0)\n",
    "\n",
    "    train_agent_dur = train_data.groupby('agent')['duration'].sum()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "    'train_duration': train_agent_dur,\n",
    "        }).fillna(0)\n",
    "\n",
    "    q1_value = df['train_duration'].quantile(0.16)\n",
    "\n",
    "    df_top_q1 = df[df['train_duration'] >= q1_value]\n",
    "\n",
    "    return df_top_q1.index\n",
    "\n",
    "\n",
    "def split_by_time_bins(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    df['num_days'] = (df['finished_at'].dt.normalize() - df['started_at'].dt.normalize()).dt.days + 1\n",
    "    \n",
    "    df_exploded = df.loc[df.index.repeat(df['num_days'])].copy()\n",
    "    df_exploded['day_offset'] = df_exploded.groupby(level=0).cumcount()\n",
    "    \n",
    "    df_exploded['current_day_midnight'] = df_exploded['started_at'].dt.normalize() + pd.to_timedelta(df_exploded['day_offset'], unit='D')\n",
    "    \n",
    "    df_exploded['started_at'] = df_exploded[['started_at', 'current_day_midnight']].max(axis=1)\n",
    "    df_exploded['day_end_boundary'] = df_exploded['current_day_midnight'] + pd.to_timedelta(1, unit='D')\n",
    "    df_exploded['finished_at'] = df_exploded[['finished_at', 'day_end_boundary']].min(axis=1)\n",
    "    \n",
    "    df_daily = df_exploded[df_exploded['started_at'] < df_exploded['finished_at']].reset_index(drop=True)\n",
    "\n",
    "    time_bins = [\n",
    "        ('00:00:00', '06:00:00', 'Early Morning'),  # 0:00 - 5:59\n",
    "        ('06:00:00', '09:00:00', 'Morning Rush'),   # 6:00 - 8:59\n",
    "        ('09:00:00', '14:00:00', 'Mid Day'),        # 9:00 - 13:59\n",
    "        ('14:00:00', '17:30:00', 'Afternoon'),      # 14:00 - 17:29\n",
    "        ('17:30:00', '21:30:00', 'Evening'),        # 17:30 - 21:29\n",
    "        ('21:30:00', '1 day',    'Night')           # 21:30 - 23:59\n",
    "    ]\n",
    "    \n",
    "    final_segments = []\n",
    "\n",
    "    for start_str, end_str, label in time_bins:\n",
    "        temp_df = df_daily.copy()\n",
    "        \n",
    "        bin_start_delta = pd.to_timedelta(start_str)\n",
    "        bin_end_delta = pd.to_timedelta(end_str)\n",
    "        \n",
    "        bin_abs_start = temp_df['current_day_midnight'] + bin_start_delta\n",
    "        bin_abs_end = temp_df['current_day_midnight'] + bin_end_delta\n",
    "        \n",
    "        temp_df['started_at'] = pd.concat([temp_df['started_at'], bin_abs_start], axis=1).max(axis=1)\n",
    "        temp_df['finished_at'] = pd.concat([temp_df['finished_at'], bin_abs_end], axis=1).min(axis=1)\n",
    "        \n",
    "        valid_segments = temp_df[temp_df['started_at'] < temp_df['finished_at']]\n",
    "        final_segments.append(valid_segments)\n",
    "\n",
    "    df_split = pd.concat(final_segments).sort_values(by=['started_at']).reset_index(drop=True)\n",
    "    \n",
    "    cols_to_drop = ['num_days', 'day_offset', 'current_day_midnight', 'day_end_boundary']\n",
    "    df_split = df_split.drop(columns=[c for c in cols_to_drop if c in df_split.columns])\n",
    "\n",
    "    return df_split\n",
    "\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points on Earth (in km)\n",
    "    using the Haversine formula.\n",
    "    \"\"\"\n",
    "    # Earth radius in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "    \n",
    "    # Calculate differences\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def assign_time_segment(dt):\n",
    "    \"\"\"\n",
    "    Assign time segment based on hour and minute.\n",
    "    Segments: 0-5.59, 6-8.59, 9-13.59, 14-17.29, 17.30-21.29, 21.30-23.59\n",
    "    \"\"\"\n",
    "    hour = dt.hour\n",
    "    minute = dt.minute\n",
    "    \n",
    "    if hour < 6:\n",
    "        return '0-5.59'\n",
    "    elif hour < 9:\n",
    "        return '6-8.59'\n",
    "    elif hour < 14:\n",
    "        return '9-13.59'\n",
    "    elif hour < 17 or (hour == 17 and minute < 30):\n",
    "        return '14-17.29'\n",
    "    elif hour < 21 or (hour == 21 and minute < 30):\n",
    "        return '17.30-21.29'\n",
    "    else:\n",
    "        return '21.30-23.59'\n",
    "\n",
    "def data_processing(path, residents):\n",
    "    \n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    data = data[data.agent.isin(residents)]\n",
    "\n",
    "    data['started_at'] = pd.to_datetime(data['started_at'])\n",
    "    data['started_at'] = data['started_at'].dt.tz_localize('UTC').dt.tz_convert('Asia/Tokyo')\n",
    "    data['finished_at'] = pd.to_datetime(data['finished_at'])\n",
    "    data['finished_at'] = data['finished_at'].dt.tz_localize('UTC').dt.tz_convert('Asia/Tokyo')\n",
    "    \n",
    "    data = split_by_time_bins(data.copy())\n",
    "\n",
    "    data['duration_min'] = (data['finished_at'] - data['started_at']).dt.total_seconds() / 60.0\n",
    "    data['duration'] = data['duration_min'].clip(lower=0).fillna(0)\n",
    "\n",
    "\n",
    "    dur = data.groupby(['agent', 'location_id'])['duration'].sum()\n",
    "    homes = dur.groupby('agent').idxmax()\n",
    "    homes = homes.apply(lambda x: x[1])\n",
    "    homes_df = homes.reset_index()\n",
    "    homes_df.columns = ['agent', 'home_location_id']\n",
    "\n",
    "\n",
    "\n",
    "    home_coords_unique = (\n",
    "    data.groupby(['agent', 'location_id'], as_index=False)\n",
    "        .agg(\n",
    "            home_latitude=('latitude', 'first'),\n",
    "            home_longitude=('longitude', 'first')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Merge homes_df (one row per agent) with unique coordinates\n",
    "    homes_df = homes_df.merge(\n",
    "    home_coords_unique,\n",
    "    left_on=['agent', 'home_location_id'],\n",
    "    right_on=['agent', 'location_id'],\n",
    "    how='left',\n",
    "    validate='one_to_one'  # <-- will raise if you mess this up again\n",
    "    )\n",
    "\n",
    "    homes_df = homes_df[['agent', 'home_location_id', 'home_latitude', 'home_longitude']]\n",
    "\n",
    "    # Now this is safely one-to-one on agent:\n",
    "    data = data.merge(homes_df, on='agent', how='left', validate='many_to_one')\n",
    "\n",
    "    # Calculate distance from home for each staypoint\n",
    "    data['distance_from_home'] = data.apply(\n",
    "    lambda row: haversine_distance(\n",
    "        row['latitude'], row['longitude'],\n",
    "        row['home_latitude'], row['home_longitude']\n",
    "    ), axis=1\n",
    "    )\n",
    "\n",
    "    # Assign time segments\n",
    "    data['time_segment'] = data['started_at'].apply(assign_time_segment)\n",
    "\n",
    "    data['day_of_week'] = data['started_at'].dt.dayofweek\n",
    "    data['day_type'] = data['day_of_week'].apply(lambda x: 'weekend' if x >= 5 else 'weekday')\n",
    "\n",
    "    data = data.sort_values(['agent', 'started_at']).reset_index(drop=True)\n",
    "\n",
    "    print(\"Calculating speeds between staypoints...\")\n",
    "    data['speed'] = np.nan\n",
    "\n",
    "    for agent in data['agent'].unique():\n",
    "        agent_mask = data['agent'] == agent\n",
    "        agent_indices = data[agent_mask].index.tolist()\n",
    "        agent_data = data.loc[agent_indices].copy()\n",
    "        \n",
    "        for i in range(1, len(agent_data)):\n",
    "            prev_idx = agent_indices[i-1]\n",
    "            curr_idx = agent_indices[i]\n",
    "            \n",
    "            prev_row = data.loc[prev_idx]\n",
    "            curr_row = data.loc[curr_idx]\n",
    "            \n",
    "            if prev_row['time_segment'] == curr_row['time_segment']:\n",
    "                # Calculate distance between locations\n",
    "                distance = haversine_distance(\n",
    "                    prev_row['latitude'], prev_row['longitude'],\n",
    "                    curr_row['latitude'], curr_row['longitude']\n",
    "                )\n",
    "                \n",
    "                time_diff = (curr_row['started_at'] - prev_row['finished_at']).total_seconds() / 3600.0\n",
    "                \n",
    "                if time_diff > 0:\n",
    "                    speed = distance / time_diff\n",
    "                    data.at[curr_idx, 'speed'] = speed\n",
    "\n",
    "    print(\"Speed calculation completed.\")\n",
    "    data.fillna(0, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def build_profiles(data):\n",
    "\n",
    "    print(\"Building agent profiles...\")\n",
    "\n",
    "    profiles = []\n",
    "\n",
    "    segment_required_minutes = {\n",
    "    '0-5.59': 6 * 60,          # 360\n",
    "    '6-8.59': 3 * 60,          # 180\n",
    "    '9-13.59': 5 * 60,         # 300\n",
    "    '14-17.29': int(3.5 * 60), # 210\n",
    "    '17.30-21.29': 4 * 60,     # 240\n",
    "    '21.30-23.59': int(2.5 * 60)  # 150\n",
    "}\n",
    "\n",
    "    for agent in data['agent'].unique():\n",
    "        agent_data = data[data['agent'] == agent]\n",
    "        \n",
    "        for day_type in ['weekday', 'weekend']:\n",
    "            day_data = agent_data[agent_data['day_type'] == day_type]\n",
    "            \n",
    "            for segment in ['0-5.59', '6-8.59', '9-13.59', '14-17.29', '17.30-21.29', '21.30-23.59']:\n",
    "                segment_data = day_data[day_data['time_segment'] == segment]\n",
    "                \n",
    "                if len(segment_data) > 0:\n",
    "\n",
    "                    # segment_data['date'] = segment_data['started_at'].dt.date\n",
    "                    # duration_per_day = segment_data.groupby('date')['duration'].sum()\n",
    "\n",
    "                    # required = segment_required_minutes[segment]\n",
    "                    # threshold = 0.5 * required  # 60%\n",
    "\n",
    "                    # # days that have enough coverage\n",
    "                    # valid_days = duration_per_day[duration_per_day >= threshold].index\n",
    "\n",
    "                    # # keep only rows from valid days\n",
    "                    # segment_data = segment_data[segment_data['date'].isin(valid_days)]\n",
    "\n",
    "                    # if after filtering nothing remains, skip\n",
    "                    if len(segment_data) == 0:\n",
    "                        continue\n",
    "\n",
    "                    period_start = segment_data['started_at'].min()\n",
    "                    period_end = segment_data['started_at'].max()\n",
    "\n",
    "                    # 1. Unique location IDs\n",
    "                    unique_locations = segment_data['location_id'].nunique()\n",
    "                    \n",
    "                    # 2. Average distance from home\n",
    "                    avg_distance_from_home = round(segment_data['distance_from_home'].mean(), 2)\n",
    "                    \n",
    "                    # 3. Average speed between staypoints (only for rows with speed calculated)\n",
    "                    avg_speed = round(segment_data['speed'].mean(),2)\n",
    "\n",
    "                    # 4. Array of unique locations\n",
    "                    unique_loc = segment_data['location_id'].unique()\n",
    "\n",
    "                    # 5. Max Stay duration in one staypoint\n",
    "                    mean_of_daily_max = round(segment_data.groupby(segment_data['started_at'].dt.date)['duration'].max().mean(), 2)\n",
    "\n",
    "                    # 6. mean transformations of staypoints within a day\n",
    "                    transformations = segment_data.groupby(segment_data['started_at'].dt.date)['location_id'].count().mean()\n",
    "                    transformations = math.ceil(transformations)\n",
    "\n",
    "                    # 7. Max distance from home\n",
    "                    max_distance_from_home = round(segment_data['distance_from_home'].max(), 2)\n",
    "                    \n",
    "                    # 8. Domanent staypoint Category\n",
    "                    dominent_poi = segment_data['poi_category'].value_counts().idxmax()\n",
    "\n",
    "                    # 9. Dictionary of POIs visited\n",
    "                    poi_dict = segment_data['poi_category'].unique()\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    profiles.append({\n",
    "                        's_date': period_start,\n",
    "                        'e_date': period_end,\n",
    "                        'agent': agent,\n",
    "                        'day_type': day_type,\n",
    "                        'time_segment': segment,\n",
    "                        'unique_location_ids': unique_locations,\n",
    "                        'avg_distance_from_home_km': avg_distance_from_home,\n",
    "                        'avg_speed_kmh': avg_speed,\n",
    "                        'unique_locs': unique_loc,\n",
    "                        'max_stay_duration': mean_of_daily_max,\n",
    "                        'transformations': transformations,\n",
    "                        'max_distance_from_home': max_distance_from_home,\n",
    "                        'dominent_poi': dominent_poi,\n",
    "                        'poi_dict': poi_dict\n",
    "                    })\n",
    "\n",
    "    agent_profiles = pd.DataFrame(profiles)\n",
    "    print(f\"Profiles created for {agent_profiles['agent'].nunique()} agents\")\n",
    "    print(f\"Total profile entries: {len(agent_profiles)}\")\n",
    "\n",
    "    agent_profiles.fillna(0, inplace=True)\n",
    "\n",
    "    return agent_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b3ffc",
   "metadata": {},
   "source": [
    "### Monthly Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "580b9d2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train = '/Users/chanuka/Desktop/codespaces/liad/processed/train.csv'\n",
    "test = '/Users/chanuka/Desktop/codespaces/liad/processed/test.csv'\n",
    "\n",
    "residents = agent_type_filter(train)\n",
    "train_data = data_processing(train, residents)\n",
    "train_agent_profiles = build_profiles(train_data)\n",
    "train_agent_profiles.to_csv('../processed/train_monthly.csv', index=False)\n",
    "\n",
    "test_data = data_processing(test, residents)\n",
    "test_agent_profiles = build_profiles(test_data)\n",
    "test_agent_profiles.to_csv('../processed/test_monthly.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0f9c1",
   "metadata": {},
   "source": [
    "#### Weekly profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75404b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating speeds between staypoints...\n",
      "Speed calculation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/9vjz2c7146g2qrp74c9hmzmm0000gn/T/ipykernel_2465/747417621.py:3: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  test_data['week'] = test_data['started_at'].dt.to_period('W').astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing week: 2024-11-18/2024-11-24, rows: 475503\n",
      "Building agent profiles...\n",
      "Profiles created for 7994 agents\n",
      "Total profile entries: 95842\n",
      "Processing week: 2024-11-25/2024-12-01, rows: 476710\n",
      "Building agent profiles...\n",
      "Profiles created for 7991 agents\n",
      "Total profile entries: 95792\n",
      "Processing week: 2024-12-02/2024-12-08, rows: 476015\n",
      "Building agent profiles...\n",
      "Profiles created for 7992 agents\n",
      "Total profile entries: 95765\n",
      "Processing week: 2024-12-09/2024-12-15, rows: 475720\n",
      "Building agent profiles...\n",
      "Profiles created for 7992 agents\n",
      "Total profile entries: 95766\n",
      "Processing week: 2024-12-16/2024-12-22, rows: 348821\n",
      "Building agent profiles...\n",
      "Profiles created for 7992 agents\n",
      "Total profile entries: 47952\n",
      "Calculating speeds between staypoints...\n",
      "Speed calculation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/9vjz2c7146g2qrp74c9hmzmm0000gn/T/ipykernel_2465/747417621.py:23: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  train_data['week'] = train_data['started_at'].dt.to_period('W').astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing week: 2024-10-14/2024-10-20, rows: 327852\n",
      "Building agent profiles...\n",
      "Profiles created for 7995 agents\n",
      "Total profile entries: 95823\n",
      "Processing week: 2024-10-21/2024-10-27, rows: 476083\n",
      "Building agent profiles...\n",
      "Profiles created for 7996 agents\n",
      "Total profile entries: 95857\n",
      "Processing week: 2024-10-28/2024-11-03, rows: 477470\n",
      "Building agent profiles...\n",
      "Profiles created for 7996 agents\n",
      "Total profile entries: 95843\n",
      "Processing week: 2024-11-04/2024-11-10, rows: 475938\n",
      "Building agent profiles...\n",
      "Profiles created for 7996 agents\n",
      "Total profile entries: 95869\n",
      "Processing week: 2024-11-11/2024-11-17, rows: 475988\n",
      "Building agent profiles...\n",
      "Profiles created for 7996 agents\n",
      "Total profile entries: 95821\n"
     ]
    }
   ],
   "source": [
    "test_data = data_processing(test, residents)\n",
    "\n",
    "test_data['week'] = test_data['started_at'].dt.to_period('W').astype(str)\n",
    "\n",
    "test_profiles = []\n",
    "i = 0\n",
    "\n",
    "for wk, chunk in test_data.groupby('week'):\n",
    "\n",
    "    print(f\"Processing week: {wk}, rows: {len(chunk)}\")\n",
    "    prof = build_profiles(chunk)\n",
    "    prof['chunk'] = i\n",
    "    test_profiles.append(prof)\n",
    "    i = i + 1\n",
    "\n",
    "test_agent_profiles = pd.concat(test_profiles, ignore_index=True)\n",
    "test_agent_profiles.to_csv('../processed/test_weekly.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "train_data = data_processing(train, residents)\n",
    "\n",
    "train_data['week'] = train_data['started_at'].dt.to_period('W').astype(str)\n",
    "\n",
    "train_profiles = []\n",
    "i = 0\n",
    "\n",
    "for wk, chunk in train_data.groupby('week'):\n",
    "\n",
    "    print(f\"Processing week: {wk}, rows: {len(chunk)}\")\n",
    "    prof = build_profiles(chunk)\n",
    "    prof['chunk'] = i\n",
    "    train_profiles.append(prof)\n",
    "    i = i + 1\n",
    "\n",
    "train_agent_profiles = pd.concat(train_profiles, ignore_index=True)\n",
    "train_agent_profiles.to_csv('../processed/train_weekly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3137e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### Data Processing\n",
    "\n",
    "train = '/Users/chanuka/Desktop/codespaces/neural_reeb/data/trail_4_sim1_fis/sim1_fis_sp_past_trajectories_with_POI.csv'\n",
    "test = '/Users/chanuka/Desktop/codespaces/neural_reeb/data/trail_4_sim1_fis/sim1_fis_sp_future_trajectories_with_POI.csv'\n",
    "\n",
    "train_data = pd.read_csv(train)\n",
    "test_data = pd.read_csv(test)\n",
    "\n",
    "eval_b_train = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_stay_points_test_anomalous.parquet')\n",
    "poi_data = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_poi.parquet')\n",
    "eval_b_train = eval_b_train.merge(poi_data, on='poi_id', how='left')\n",
    "eval_b_train\n",
    "\n",
    "eval_b_train.rename(columns={'agent_id': 'agent', 'start_datetime':'started_at', 'end_datetime':'finished_at', 'poi_id': 'location_id', 'category':'poi_category'}, inplace=True)\n",
    "eval_b_train.drop(columns=['source', 'anomaly_type', 'anomaly'], inplace=True)\n",
    "\n",
    "gt = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail_4_sim1_fis/anomalous_temporal_sim1.csv')\n",
    "ground_truth = pd.read_csv('/Users/chanuka/Desktop/codespaces/liad/data/trail_4_sim1_fis/anomalous_agent_sim1.csv')\n",
    "\n",
    "gt\n",
    "\n",
    "eval_b_train = pd.read_parquet('/Users/chanuka/Desktop/codespaces/liad/data/files/evalb_stay_points_test_anomalous.parquet')\n",
    "eval_b_train = eval_b_train[eval_b_train['anomaly'] == True]\n",
    "eval_b_train.rename(columns={'agent_id': 'agent', 'start_datetime': 'anomaly_start_time', 'end_datetime': 'anomaly_end_time'}, inplace=True)\n",
    "eval_b_train.to_csv('../processed/anomalous_temporal.csv', index= False)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
